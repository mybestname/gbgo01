# 第11课 DNS & CDN & 多活架构

## DNS
- DNS 介绍
- DNS 问题
- 高可用的 DNS 设计 
- 高可用 DNS 的最佳实践

## DNS 介绍
- DNS(Domain Name System，域名系统)，DNS 服务用于在网络请求时，将域名转为IP地
址。能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的 IP 数串。 
- 传统的基于 UDP 协议的公共 DNS 服务极易发生 DNS 劫持，从而造成安全问题。

- 递归查询
  - 如果主机所询问的本地域名服务器不知道被查询域名的 IP 地址，
  - 那么本地域名服务器 就以 DNS 客户的身份，向其他根域名服务器继续发出查询请求报文，
  - 而不是让该主机自己进行下一步的查询。
- 迭代查询
  - 当根域名服务器收到本地域名服务器发出的 迭代查询请求报文时，要么给出所要查询的 IP 地址，要么告诉本地域名服务器:你下一步应当向哪一个域名服务器进行查询。
  - 然后让本地域名服务器进行后续的查询，而不是替本地域名服务器进行后续的查询。 
  - 由此可⻅，客户端到 Local DNS 服务器， Local DNS 与上级 DNS 服务器之间属于递 归查询;DNS 服务器与根 DNS 服务器之 前属于迭代查询。

## DNS 问题 
### Local DNS 劫持:
Local DNS 把域名劫持到其他域名，实现其不可告人的目的。

### 域名缓存
- LocalDNS 缓存了业务的域名的解析结果，不向权威 DNS 发起递归。
- 保证用户访问流量在本网内消化
  - 国内的各互联网接入运营商的带宽资源、网间结算费用、IDC 机房 分布、网内 ICP 资源分布等存在较大差异。
  - 为了保证网内用户的访问质量，同时减少跨网结算，运营商在网内搭建了内容缓存服务器，通过把域名强行 指向内容缓存服务器的 IP 地址，就实现了把本地本网流量完全留在了本地的目的。
- 推送广告
  - 有部分 LocalDNS 会把部分域名解析结果的所指向的内容缓存，并替换成第三方广告联盟的广告。

### 解析转发
- 指运营商自身不进行域名递归解析，而是把域名解析请求转发到其它运营商的递归 DNS 上的行为。
- 部分小运营商为了节省资源，就直接将解析请求转发到了其它运营的递归 LocalDNS 。
- 导致权威 DNS 收到的域名解析请求的来源IP为 其它运营商 IP，
- 最终导致用户流量被导向了错误的IDC，用户访问变慢。

### LocalDNS 出口 NAT 
- 指运营商 的 LocalDNS 按照标准的 DNS 协议进行递归，但是因为在网络上存在多出口且配置了目标路由NAT，
- 结果导致 LocalDNS 最终进行递归解析的时候的 出口 IP 就有概率不为本网的 IP 地址。
- DNS 收到的域名解析请求的来源 IP 还是成了其它运营商的 IP，最终导致用户流量被导向了错误的IDC，用户 访问变慢。

## 高可用DNS设计 

### 实时监控 + 商务推动
- 这种方案就是周期比较⻓，毕竟通过行政手段来推动运营商来解决这个问题是比较耗时的。
- 另外我们通过大数据分析，得出的结论是 Top3 的问题用户均为移动互联网用户。对于这部分用 户，我们有什么技术手段可以解决以上的问题呢?
- 绕过自动分配 DNS，使用 114DNS 或 Google public DNS:
- 如何在用户侧构造域名请求:对于 PC 端的客户端来说，构造一个标准的 DNS 请求包并不算 什么难事。但在移动端要向一个指定的 LocalDNS 上发送标准的 DNS 请求包，而且要兼容各 种 iOS 和 Android 的版本的话，技术上是可行的，只是兼容的成本会很高。
- 推动用户修改配置极高:如果要推动用户手动修改 PC 的 DNS 配置的话，在 PC 端和手机客 户端的 WiFi 下面还算勉强可行。但是要用户修改在移动互联网环境下的 DNS 配置，其难度不 言而喻。

### 完全抛弃域名，自建 HTTPDNS 进行流量调度:
- 如果要采用这种这种方案的话，拿到一份准确的 IP 地址库来判断用户的归属，然后再制定个协议搭个服务来做调度，然后再对接入层做调度改造。
- 成本会比较高，尤其对于大体量，业务规模庞大的公司而言。
- 当前主流的解决方案:HTTPDNS

#### HTTPDNS 
- HTTPDNS 利用 HTTP 协议与 DNS 服务器 交互，代替了传统的基于 UDP 协议的 DNS 交互，绕开了运营商的 Local DNS，有效防止了域名劫持，提高域名解析效率。
- 由于DNS服务器端获取的是真实客户端 IP 而非 Local DNS 的 IP，能够精确定位客户端地理位置、运营商信息，从而有效改 进调度精确性。

#### HTTPDNS 基本原理

- HTTPDNS 通过 ip 直接请求 http 获取服务器 A 记录地址，不向本地运营商询问 domain 解析过程，所以从根本避免了劫持问题。

平均访问延迟下降:
- 由于是 ip 直接访问省掉了一次 domain 解 析过程。

用户连接失败率下降:
- 通过算法降低以往失败率过高的服务器排序
- 通过时间近期访问过的数据提高服务器排序
- 通过历史访问成功记录提高服务器排序

根治域名解析异常:
- 由于绕过了运营商的 LocalDNS，用户解析域名的请求通过 HTTP 协议直接透传到了
  HTTPDNS 服务器 IP，用户在客户端的域名解析请求将不会 遭受到域名解析异常的困扰。
  
调度精准:
- HTTPDNS 能直接获取到用户IP， 通过结合 IP地址库以及测速系统，可以保证将用户引导的访问最快的 IDC 节点上;

实现成本低:
- 接入HTTPDNS的业务仅需要对客户端接入层做少量改造，无需用户手机进行root/越狱
- HTTP 协议请求构造简单，兼容各版本的移动操作系统简单
- HTTPDNS的后端配置完全复用现有权威 DNS 配置，管理成本低。

#### Anycast 
- 如果只有一个 VIP，即可以增加 DNS 记录的 TTL，减少解析的延迟。
- Anycast 可以使用一个 IP，将数据路由到最的一组服务器，通过BGP宣告这个IP

存在两个问题:
- 如果某个节点承载过多的用户会过载
- BGP 路由计算可能会导致连接重置 
  
#### “稳定 Anycast” 技术 (google)

## CDN 系统
- CDN 系统架构 
- CDN 数据一致性 
- 静/动态 CDN 加速

### CDN 系统架构
- 缓存代理
  - 通过智能 DNS 的筛选，用户的请求被透明地指向离他最近的省内⻣干节点，
    最大限度的缩短用户信息的传输距离。
- 路由加速
  - 利用接入节点和中继节点或者多线节点互联互通。
- 安全保护
  - 无论面对是渗透还是 DDoS攻击，攻击的目标大都会被指向到了 CDN，
    进而**保护了源站IP**。
- 节省成本
  - CDN 节点机房只需要在当地运营商的单线机房，或者带宽相对便宜的城市，采购成本低。
- 内容路由
  - DNS系统、应用层重定向，传输层重定向。 
  - 核心：流量的重定向
  - 部署nginx (upstream 设置)
- 内容分发
  - PUSH:主动分发，内容管理系统发起，将内容从源分发到 CDN 的 Cache 节点。
  - PULL:被动分发技术，用户请求驱动，用户请求内容miss，从源中或者其他 CDN 节点中实时获取内容。
- 内容存储
  - 随机读、顺序写、小文件的分布式存储。
  - 环形存储，COSS
  > The Cyclic Object Storage Scheme (coss) is an attempt to develop a custom filesystem for Squid
  > - http://etutorials.org/Server+Administration/Squid.+The+definitive+guide/Chapter+8.+Advanced+Disk+Cache+Topics/8.6+The+coss+Storage+Scheme/
  > - https://blog.csdn.net/adamska0104/article/details/51013245
- 内容管理
  - 提高内容服务的效率，提高 CDN 的缓存利用率。

### CDN 数据一致性
- PUSH
  - 不存在数据一致性问题。
- PULL
  - 缓存更新不及时，可设置缓存的失效时间，可以达到最终一致性。
  - 如果用户对一致性要求比较高也可以使用 `?version=xx` 的技术，
     - 也可以每次上传图片返回的url是不同的方式来代替版本号。
  > - 文件命名本身就考虑版本号，这样升版时候减小cache不刷新的问题。
  > - 前端的增量发布（让文件名本身带md5摘要）
  >   - 每次发布，文件名永远不冲突。

### CDN 数据一致性和HTTP头
- `Expires`
  - 即在 HTTP 头中指明具体失效的时间(HTTP/1.0) 
- `Cache Control`
  - max-age 在 HTTP 头中按秒指定失效的时间，优先级高于`Expires`(HTTP/1.1)
- `Last-Modified` / `If-Modified-Since`
  - 文件最后一次修改的时间(精度是秒，HTTP/ 1.0)，需要 Cache-Control 过期。
- `Etag`
  - 当前资源在服务器的唯一标识(生成规则由服务 器决定)优先级高于`Last-Modified`


### 静态 CDN 加速

- 静态域名非主域名
  - 不要使用API的主域名。
    - 静态资源不使用cookie，浪费流量。
    - 可能的安全隐患 
- 静态多域名和收敛
  - PC端：静态域名多搞 (Http1.1，每域名4～5连接，多域名提高并发)
  - 移动端：相反，需要减少DNS解析成本，静态域名需要收敛。  
- 静态资源版本化管理
  - 需要做好静态域名的增量发布。

### 动态 CDN 加速 
> 动态和静态的核心区别
>  - 静态访问同一资源，命中直接返回。 (css,js等静态文件)
>  - 动态相当于对API进行加速。相当于永远都不命中缓存，每次都回源。
- TCP 优化
  - 设计算法来处理网络拥堵和包丢失，加快这些情况下的数据从CDN 的恢复以及一些常⻅的 TCP 瓶颈。
    - 更激进的TCP滑动窗口算法 
      - TCP的快速重连
      - TCP的BBR
    > BBR
    >  - https://github.com/google/bbr 
- Route optimization
  - 优化从源到用户端的请求的线路，以及可靠性，就是不断的测量计算得到更快更可靠的路线。
- Connection management
  - 就是边缘和源之间，包括 CDN 之前的线路，采用⻓连接，而不是每一个请求一个连接
    - 减小核心机房入口SLB的成本
- On-the-fly compression
  - 就是数据在刚刚离开源的时候就进行压缩，可以缩短 在整个网络之中的流通时间。
- SSL offload
  - 加速或者说减少一些安全监测，减少原服务器执行这 种计算密集型的压力。

## 多活
- 多活系统
- 账号多活
- 稿件多活

### 多活策略
- 业务分级
   - 按照一定的标准将业务进行分级，挑选出**核心业务**，
   - 只为核心业务核心场景设计异地多活，降低方案整体复杂度和实现成本。
   - 例如
     - 访问量
     - 核心场景 
     - 收入
   - 避免所有业务全部多活，分阶段分场景推进。
- 数据分类
   - 挑选出核心业务后，需要对核心业务相关的数据进一步分析，目的在于识别所有的数据及数据特征， 
   - 这些数据特征会影响后面的方案设计。
   - 常⻅的数据特征分析维度有:
      - 数据量
      - 唯一性
      - 实时性
      - 可丢失性
      - 可恢复性
- 数据同步
   - 确定数据的特点后，可以根据不同的数据设计不同的同步方案。
   - 常⻅的数据同步方案有:
      - 存储系统同步
      - 消息队列同步
      - 重复生成
- 异常处理
   - 无论数据同步方案如何设计，一旦出现极端异常的情况，总是会有部分数据出现异常。
   - 例如，同步延迟、数据丢失、数据不一致等。
   - 异常处理就是假设在出现这些问题时，系统将采取什么措施来应对。 
   - 常⻅的异常处理措施
     - 多通道同步
     - 同步和异步访问 
     - 日志记录 
     - 补偿

### 多活术语
- RPO(Recovery Point Object)
  - 表示机房级别故障时，未被同步的数据时⻓。
  - MySQL设置
    - 特殊情况 复制延迟较大情况 RPO 设置为分钟级别，
    - 正常情况 RPO 为秒级
- RTO(Recovery Target Object)
  - 表示机房故障情况下，关键流程或系统切换恢复时间
  - 一般为分钟级别
- WRT(Work Recovery Time)
  - 表示故障时，由于 RPO 导致的未同步异常数据修复完成时⻓
  - 一般为小时级别。

### B站多活
  - B站业务以观看体验为主（对比淘宝，以交易单元、买家为维度）。
     - 核心需求
          - PC/APP 首⻚可观看
          - 视频详情⻚可打开
          - 账号可登陆、鉴权
     - 根据核心需求，将多活资源分为三类:
       - Global 资源
         - 多个 Zone(机房)共享访问的资源，每个 Zone 访问本 Zone 的资源，
         - Global 层面来说是单写 Core Zone(核心机房)，即:**单写+多读**
         - 利用数据复制(写Zone 单向)实现最终一致性方案实现;
       - Multi Zone 资源
         - 多个 Zone 分片部署，每个 Zone 拥有部分的 Shard 数据
         - 比如**按照用户维度拆分**，用户 A 可能在 ZoneA，用户 B 可能在 ZoneB，
         - 即:**多写+多读**、利用数据复制 (写 Zone 双向复制)方案实现;
       - Single Zone 资源
         - 单机房部署业务 
     - 观看类业务最合适的场景
       - 采用 Global 资源策略
       - 对于社区类(评论、弹幕)采用 Multi Zone 策略。



### 饿了么多活
- 核心业务
  - 3个最重要的⻆色，用户、商家和骑手
  - 订单3步骤: 
     - 1.用户下单：用户APP，系统推荐，推荐顺序中结合了用户习惯，推荐排序，商户的推广等。
          用户找到中意的⻝物 ，下单并支付，订单流转到商家。 
     - 2.商家接单：商家接单并开始制作⻝物，制作完成后，系统调度骑手赶到店面，取走⻝物。
     - 3.骑手配送：骑手按照配送地址，把⻝物送到客户手中。

- 业务可内聚: 
  - 业务是地域化，通过合理的地域划分，能够实现业务内聚。
  - 每个机房为一个 ezone，一个 ezone 包含了饿了么需要的各种服务。
    - 一笔业务内聚在一个 ezone 中，一个定单涉及的用户，商家，骑手，都在相同的机房，
    - 订单在各个⻆色之间流转速度最快，不会因为各种异常情况导致延时。
    - 单个订单的旅单过程，在一个机房中完成，不允许跨机房调用。
    - 保证实时性，旅单过程中不依赖另外一个机房的服务，保证没有延迟。
  > **业务特点决定了多活策略选择**

- 可用性优先
  - 发生故障切换机房时，优先保证系统可用，
    - 首先让用户可以下单吃饭，容忍有限时间段内的数据不一致，
  - 每个 ezone 都会有全量的业务数据
  - 一个 ezone 失效后，其他的 ezone 可以接管用户。
  - 用户在一个ezone的下单数据， 会实时的复制到其他ezone。

- 保证数据正确
  - 在确保可用的情况下，需要对数据做保护以避免错误，
  - 在切换和故障时， 如果发现某些订单的状态在两个机房不一致，
  - 会锁定该笔订单，阻止对它进行更改，保证数据的正确。
    - 人工介入
    
- 业务可感
  - 基础设施还没有强大到可以抹去跨机房的差异，需要让业务感知多活逻辑，
  - 业务代码要做一些改造
    - 能够识别出业务数据的归属，只处理本 ezone 的数据，过滤掉无关的数据。
    - 完善业务状态机，能够在数据出现不一致的时候，通过状态机发现和纠正。

- Sharing方法 
  - 实现业务内聚，选择划分方法(Sharding Key)，对服务进行分区，让用户，商户，骑手能够正确的内聚到同一个 ezone 中。
  - 分区方案是整个多活的基础，它决定了之后的所有逻辑。
  - 根据业务特点，
    - 选择地理位置作为划分单元 地理围栏
      - 地理围栏主体按照省界划分，再加上局部微调
    - 地理位置上接近的用户，商户，骑手划分到同一个 ezone，一个订单的履单流程在一个机房完成，保证最小的延时
    - 某个机房出现问题时，按照地理位置把用户，商户， 骑手打包迁移到别的机房。

- API Router
  - 基于地理位置划分的流量路由层
  - 负责对客户端 API 调用进行路由，把流量导向到正确的 ezone。
  - API Router 部署在多个公有云机房中， 用户就近接入到公有云的API Router，还可以提升接入质量。
  - 最基础的分流标签是地理位置，，AR 就地理位置 计算出正确的 shard 归属。
  - 业务复杂，并不是所有的调用都能直接关联到某个地理位置
     - 使用分层的路由方案，
       - 核心的路由逻辑是地理位置，
       - 支持其他的一些 High Level Sharding Key，
       - 由 APIRouter 转换为核心的 Sharding Key，
- SOA Proxy
  - 用于路由SOA调用的，和API Router基于相同的路由规则。

### 阿里多活

- 架构
  - GZone (gloabl zone): 不可拆分的数据和服务
  - RZone (region zone)：按用户分片为5个，每个分片5个副本，只有一个可以写，其它Paxos一致性。
  - CZone (city zone)：GZone的只读副本。RZone逻辑上依赖GZone，物理上本市的RZone访问本市的CZone。

- 业务原则
  - 按买家维度数据切片
    - 不能按卖家，要保证买家的用户体验。
    - 买家远大于卖家。
  - 只取买家链路相关业务（单元）做"多活"
    - 卖家数据变成Global zone，（单机房单写）卖家数据复制
  - 业务（单元）内最大限度封闭
  - 如无法接受最终一致性的业务，使用跨单元的单点写。

- 容灾
  - 同城容灾
    - RZone1出现故障，目标将RZone1切换至同城容灾 RZone2。
      - 先做数据库分片切换， RZone1对应的分片为分片1，把分片1在RZone2的副本提升为主副本，
      - 数据库副本提升完毕后将 RZone1的流量切换至RZone2，
      - 实现同城容灾RPO=0、RTO<1min。
   - 异地容灾
    - RZone1故障。目标切换至异地RZone3
    - 先做数据库切换，分片1在RZone3的副本切换成主副本， 
    - 完成后将RZone1的流量切换至 RZone3，实现异地容灾，
    - 该过程 RPO=0、RTO<1min。

- 流量路由
  - 模块核心是将用户的 uid 信息和对应的 Zone 信息植入到 cookie 中，供路由模块做精准路由。
  - 服务路由 分为 本机房服务路由 和 跨机房服务路由 调用
    - 本机房服务路由
      - 服务调用端向本机房服务注册中心订阅服务，发现服务地址后做本机房服务路由调用。
    - 跨机房服务路由调用
      - 服务调用端向其他 IDC 的注册中心订阅服务地址，发现服务地址后做跨机房服务调用。

- OceanBase保证的强一致性
  - 每个分片的数据库做5副本部署，部署地域实现三地五中心部署，
  - 5副本中有3副本实现强一致，可以实现同城、IDC 容灾和异地容灾。
  > 注意 Paxos/raft协议要求过半机器写成功

### 苏宁多活
- 架构
  - Cell:业务可封闭收敛最小执行分片
    - 业务对请求 空间按一定维度(比如会员、⻔店等)划分分片。
  - LDC:逻辑数据中心，
    - 由多个业务可封闭 cell 组成的集合单元，拥有独立的基础中间件系统(包 括 RPC， MQ， DNS 等)，以及出口网络等。
  - PDC:物理数据中心，
    - 指物理上独立的一栋建筑，一般每栋有好几层, 存放一系列机柜和上千和 上万服务器, 构成一个 PDC。
  - AZ(Available Zone):可用区，
    - 具有独立的故障隔离空间，拥有独立网络设施或电力设备，由相邻的单个或多个 PDC 组成。
  - Region:地理区域，
  - 有多可用区所组成的集合， 区域之间故障域完全隔离。

- 业务原则
  - 分片服务
    - 对应的数据仅在某个 Cell 存在， 其它 Cell 不与交叉或共享，比如会员服务、订单服务等。
  - 共享服务:
    - 所有 Cell 拥有相同的数据，相互共享，比如价格服务、商品服务等。
  - 索引服务
    - 用于索引数据提供服务，类似共享服务。
  - 竞争 (控制) 服务
    - 各个 Cell 相互操作同一个数据，为了保证数据一致性，需要在同一个数据中心进行控制，比如库存的扣减、用户注册等。
  - 竞争 Proxy 服务
    - 用于竞争服务前置服 务，比如**库存前置调拨服务**。
  > 秒杀，限流时候Quota，通过批量Quota，减少请求Redis次数
  >  - 一次批量调拨一批库存，先本地消耗。
  >  - 减少跨机房调用和请求数据中心
  >  - 减少Global资源争用

- 容灾
  - 确保数据高可用以及任何一个机房故障都可被接管，所有数据中心都包含全量数据，
    当主数据中心的变更将会实时同步到各个从数据中心。
  - 数据中心之间，延迟较大，同步一般采用异步复制方式。
    - 在机房故障等极端情况，出现少量数据未同步到其它数据中心，
      - 机房恢复后，对未同步的数据进行人工修复。

### Facebook 
- Memcache 一致性
  - 1. 设定remote marker到缓存中。
  - 2. 数据转发到主库
  - 3. 删除缓存，因为此次有remote marker存在，读取会直接走主库。
  - 4. 数据复制到从库。
  - 5. 删除remote marker。此时因为remote marker已经删除，读取走从库。同时更新缓存
       - 换言之，缓存只从从库更新，而只有从库准备好后，缓存才可用。

### 微信朋友圈异地多活

- 分析
  - 因果关系对事件施加了一种顺序
    - 因在果之前，消息发送在消息收取之前。
    - 现实生活，一件事会顺序地导致另一件事发生
    - 某个节点读取了一些数据然后写入一些结果， 另一个节点读取其写入的内容，并依次写入一些其他内容等等。
  - 因果依赖操作链定义了系统中的因果顺序
     - 什么在什么之前发生。
  - 分布式系统的因果一致性
    - 一个系统服从因果关系所规定的顺序
  - 微信朋友圈某条状态的评论以及对评论的答复 (也是评论)所构成的因果关系。
    - 需要保证不同数据中心间的因果一致性来保证
    - 一个用户在刷朋友圈的时候不会出现看到评论所对应的答复，却看不到答复对应的评论。
  - 微信朋友圈例子：
    - 网络在不同副本间复制数据时的延迟、中断等分布式系统中常⻅的场景
    - 两条消息在同步到用户Kate(加拿大)所在数据中心上的副本时已经乱序。
      ```
      上海副本（小王）
        “Mary:这是哪里?”
        “小王:Mary，这是梅里雪山”，
      香港副本（mary）
        “Mary:这是哪里?”
        “小王:Mary，这是梅里雪山”，
      加拿大副本（Kate） 
        “小王:Mary，这是梅里雪山”
        “Mary:这是哪里?”
      ```
    - 解决办法
      - “Mary:这是哪里?” -> **因**
      - “小王:Mary，这是梅里雪山” -> **果**
      - 同步到 Kate 所在的数据中心副本时发生乱序，
        - Kate 刷朋友圈时，根据**因果关系**
        - 评论、答复的顺序调整到正确的、可阅读。
  
#### 基于逻辑时钟解决因果关系序列    
- 每条评论都有一个唯一的且递增ID
  - ID生成器：入口获取本IDC内唯一、递增的ID。
- 该IDC下，**每条新评论的ID都必须大于本IDC可见的全局最大ID**，确保因果关系。
  - 本例：
    - 香港IDC，当发表完评论2，已经同步上海IDC的评论 1, 4, 7
    - 香港域下的用户发表新评论，一定要大于当前能看到的全局最大ID，此时为7
    - 所以香港此时用户最新发表的评论的 ID 为8 
- 广播本地看到的所有评论和新评论到其它IDC
  - 该地域就负责申请一个全局ID，然后将这个评论的事件广播给其他的数据中心。
  - 本例
    - 香港IDC就合并1 2 4 7 8等针对同一条朋友圈状态的一系列评论事件IDs，
    - 然后再整体广播，保证针对同一条状态的所有当前最新的事件整体被广播
      - 香港IDC只广播8
      - 如果前面的事件序列在广播的中途丢失，
      - 那么其他节点比如加拿大IDC 就会漏掉部分评论事件，
      - 这也是数据多重补位的措施。全量冗余广播。
      - 前提（数据冗余不多）
        - 因为同一个朋友圈的发布状态，一般的评论不会很多
        - 所以造成的数据冗余交互不会很大，
        - 否则是不行的。
- 相同ID的评论合并排重
  - 本例
    - 加拿大IDC 会收到 来自上海IDC 的1 4 7事件系列
    - 也会收到来自香港 IDC 同步过来的1 4 7 8 事件系列
    - 两个广播的事件系列有重复，需要去重。

> 逻辑时钟（vector clock/lamport timestamp)
> - https://en.wikipedia.org/wiki/Vector_clock
> - https://en.wikipedia.org/wiki/Lamport_timestamp

## 账号多活
### 概念和误区
- A中心注册了用户，数据还未同步到 B 中心，
  - 此时 A 中心宕机，支持注册业务多活，挑选 B 中心让用户去重新注册。
- 问题
  - 一个手机号只能注册一个账号，A 中心的数据没有同步过来，
  - B 中心无法判 断这个手机号是否重复，
     - 如果 B 中心让用户注册， 后来 A 中心恢复了，发现数据有冲突，怎么解决?
     - 实际上是无解
       - 因为注册账号不能说挑选最后一个生效
       - 如果 B 中心不支持本来属于 A 中心的业务进行注册，注册业务的双活又成了空谈。
- 会不自觉**思维误区**
  - 我要保证所有业务的“异地多活”!
  - 账户本来就不能多活！账户本来就应该是一个GlobalZone! 无法分割的业务或数据。
  - 真正要搞的是支持"登陆"和"鉴权"多活！而不是"注册"多活。（即写暂时不可用是可以忍受的！）

### 减少数据同步
- 用户登录所产生的 token 或者 session 信息，数据量很大，
  - 但其实很多情况**不需要同步**到其它业务中心，因为这些**数据丢失后重新登录**就可以了。
- 可能出现消息队列的同步延迟 (非同步的情况下，可以使用二次读取)
  - 用户在 A 中心注册，然后访问 B 中心的业务，
  - 此时B 中心本地拿不到用户的账号数据。
  - 为了解决这个问题，
     - B 中心在读取本地数据失败的时候，可以根据路由规则， 再去A 中心访问一次
     - 这就是所谓的二次读取，第一次读取本地，本地失败后第二次读取对端。
- 由于数据量很大，我们可以不同步数据的情况 (完全不同步的情况，可以回源)
  - 当用户在 A 中心登录后，然后又 在 B 中心登录，
     - B 中心拿到用户上传的 session id 后，根据路由判断 session 属于 A 中心
     - 直接去 A 中心请求session数据即可，
  - 反之亦然，A 中心也可 以到 B 中心去拿取session 数据。

### 最终一致性
- A机房注册了一个用户，业务上不要求能够在50ms 内就同步到所有机房，
  - 正常情况下要求5分钟同步 到所有机房即可，
  - 异常情况下甚至可以允许1小时 或者1天后能够一致。
- 最终一致性在具体实现的时候，还需要根据不同的数据特征，进行差异化的处理，以满足业务需要。
  - 例如对“账号”信息来说，
    - 如果在 A 机房新注册的用户，B 机房还没有 这个用户的信息，
    - 为了保证业务的正确，B 机房就需要根据路由规则到 A 机房请求数据。
- 而对“用户信息”来说，5分钟后同步也没有问题，也不需要采取其它措施来弥补
- 影响用户体验，即用户看到了旧的用户信息，这个问题怎么解决呢?
  - 无法彻底解决，只能牺牲部分用户体验。
  - 不要试图所有强一致。

## 稿件多活
  - 稿件体系是典型的GlobalZone，即单写。
  - 核心机房负责写
    - 副本（从库）同步一定存在延时。
    - 在CDN层，就直接把写操作转到固定的机房。  
      - 运维需要精细配接口，运维有些工作量。
      - 后续可能会进一步改造中间件来解决。
  - 缓存的预热和刷新
    - 使用mysql复制，通过订阅从库的binlog，刷新缓存。
    - 这里因为业务对时效不敏感，所有cache的失效不是大问题。但不能出现不一致性。
 

## References

### 饿了么
- http://afghl.github.io/2018/02/11/distributed-system-multi-datacenter-1.html 
- https://zhuanlan.zhihu.com/p/32009822 
- https://zhuanlan.zhihu.com/p/32587960 
- https://zhuanlan.zhihu.com/p/33430869 
- https://zhuanlan.zhihu.com/p/34958596

### OPPO 
- https://mp.weixin.qq.com/s/RQiurTi_pLkmIg_PSpZtvA 
- https://mp.weixin.qq.com/s/LCn71j3hgm5Ij5tHYe8uoA 

### Facebook
- https://zhuanlan.zhihu.com/p/20827183

### 阿里
- https://www.cnblogs.com/davidwang456/articles/8192860.html 
- https://mp.weixin.qq.com/s/ty5GltO9M648OXSWgLe_Sg
- https://developer.aliyun.com/article/57715
- https://toutiao.io/posts/y8qekd/preview 
- https://tech.antfin.com/community/articles/922

### 苏宁
- https://mp.weixin.qq.com/s/WK8N4xFxCoUvSpXOwCVIXw

### 其它
- https://mp.weixin.qq.com/s/ooPLV039BAGBsiDZagWNHw 
- https://mp.weixin.qq.com/s/VPkQhJLl_ULwklP1sqF79g 
- https://mp.weixin.qq.com/s/ty5GltO9M648OXSWgLe_Sg
- https://mp.weixin.qq.com/s/GdfYsuUajWP-OWo6lbmjVQ 
- https://developer.aliyun.com/article/57715 
- https://mp.weixin.qq.com/s/RQiurTi_pLkmIg_PSpZtvA 
- https://mp.weixin.qq.com/s/LCn71j3hgm5Ij5tHYe8uoA 
- http://afghl.github.io/2018/02/11/distributed-system-multi-datacenter-1.html
- https://zhuanlan.zhihu.com/p/42150666 
- https://zhuanlan.zhihu.com/p/20827183
- https://myslide.cn/slides/733
- https://blog.csdn.net/u012422829/article/details/83718296 
- https://blog.csdn.net/u012422829/article/details/83932829 
- https://www.cnblogs.com/king0101/p/11908305.html 
- https://mp.weixin.qq.com/s/WK8N4xFxCoUvSpXOwCVIXw 
- https://mp.weixin.qq.com/s/jd9Os1OAyCXZ8rXw8ZIQmg 
- https://cloud.tencent.com/developer/article/1441455 
- https://mp.weixin.qq.com/s/RQiurTi_pLkmIg_PSpZtvA 
- https://help.aliyun.com/document_detail/72721.html 
- https://mp.weixin.qq.com/s/h_KWwzPzszrdGq5kcCudRA
- https://www.cnblogs.com/davidwang456/articles/8192860.html
- https://mp.weixin.qq.com/s/GdfYsuUajWP-OWo6lbmjVQ 
- https://www.cnblogs.com/king0101/p/11908305.html 
- https://zhuanlan.zhihu.com/p/42150666 
- https://help.aliyun.com/document_detail/72721.html 
- https://blog.csdn.net/u012422829/article/details/83718296 
- https://blog.csdn.net/u012422829/article/details/83932829 
- [异地多活的单元化设计](https://mp.weixin.qq.com/s/jd9Os1OAyCXZ8rXw8ZIQmg)
- [互联网异地多活方案发展历史](https://mp.weixin.qq.com/s?__biz=MzI3MDA2OTE0Nw==&mid=2247483726&idx=1&sn=a086311f82f24ca704286dfde97d23f9) 
- [荔枝FM架构师刘耀华：异地多活IDC机房架构](https://cloud.tencent.com/developer/article/1042081?from=article.detail.1441455)
